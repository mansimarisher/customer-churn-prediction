{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-title",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction Model\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive machine learning pipeline to predict customer churn, providing actionable insights for business strategy.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#1-data-loading)\n",
    "2. [Data Cleaning & Preprocessing](#2-data-cleaning--preprocessing)\n",
    "3. [Exploratory Data Analysis](#3-exploratory-data-analysis)\n",
    "4. [Feature Engineering](#4-feature-engineering)\n",
    "5. [Model Training & Evaluation](#5-model-training--evaluation)\n",
    "6. [Results & Insights](#6-results--insights)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "# 1. Data Loading\n",
    "\n",
    "Load the customer dataset and perform initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data file path\n",
    "DATA_FILE = '../data/customer_data.csv'\n",
    "\n",
    "def load_customer_data(file_path):\n",
    "    \"\"\"\n",
    "    Load customer data from CSV file with error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded customer data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            print(\"📝 Please place your customer data CSV file in the 'data' directory.\")\n",
    "            print(\"Expected columns: customerID, gender, SeniorCitizen, Partner, Dependents, tenure, etc.\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✅ Successfully loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_customer_data(DATA_FILE)\n",
    "\n",
    "if df is not None:\n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n🔍 First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n📈 Basic Statistics:\")\n",
    "    display(df.describe())\n",
    "else:\n",
    "    print(\"\\n📝 To proceed, please add your customer_data.csv file to the data directory.\")\n",
    "    print(\"Expected format: CSV with customer information including a 'Churn' target column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-info-header",
   "metadata": {},
   "source": [
    "## Data Information & Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Detailed data information\n",
    "    print(\"🔍 Detailed Data Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n🔄 Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\n📋 Data Types:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].dtype} ({df[col].nunique()} unique values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-cleaning-header",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Preprocessing\n",
    "\n",
    "Handle missing values, remove unnecessary columns, and prepare data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-values-header",
   "metadata": {},
   "source": [
    "## 2.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    def analyze_missing_values(df):\n",
    "        \"\"\"\n",
    "        Analyze missing values in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Missing values summary\n",
    "        \"\"\"\n",
    "        missing_data = df.isnull().sum()\n",
    "        missing_percent = (missing_data / len(df)) * 100\n",
    "        \n",
    "        missing_summary = pd.DataFrame({\n",
    "            'Missing_Count': missing_data,\n",
    "            'Missing_Percentage': missing_percent\n",
    "        })\n",
    "        \n",
    "        missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "        missing_summary = missing_summary.sort_values('Missing_Count', ascending=False)\n",
    "        \n",
    "        return missing_summary\n",
    "    \n",
    "    # Analyze missing values\n",
    "    missing_summary = analyze_missing_values(df)\n",
    "    \n",
    "    if not missing_summary.empty:\n",
    "        print(\"⚠️ Missing Values Found:\")\n",
    "        display(missing_summary)\n",
    "        \n",
    "        # Visualize missing values\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.barplot(x=missing_summary.index, y=missing_summary['Missing_Count'])\n",
    "        plt.title('Missing Values Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.barplot(x=missing_summary.index, y=missing_summary['Missing_Percentage'])\n",
    "        plt.title('Missing Values Percentage')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"✅ No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handle-missing-header",
   "metadata": {},
   "source": [
    "## 2.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    def handle_missing_values(df):\n",
    "        \"\"\"\n",
    "        Handle missing values using appropriate strategies.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with handled missing values\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Strategy for numerical columns: median imputation\n",
    "        numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        for col in numerical_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                median_value = df_clean[col].median()\n",
    "                df_clean[col].fillna(median_value, inplace=True)\n",
    "                print(f\"📊 Filled {col} missing values with median: {median_value}\")\n",
    "        \n",
    "        # Strategy for categorical columns: mode imputation\n",
    "        categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                mode_value = df_clean[col].mode()[0]\n",
    "                df_clean[col].fillna(mode_value, inplace=True)\n",
    "                print(f\"📝 Filled {col} missing values with mode: {mode_value}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean = handle_missing_values(df)\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"\\n✅ Missing values after cleaning: {remaining_missing}\")\n",
    "    \n",
    "    print(f\"\\n📊 Dataset shape after cleaning: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remove-columns-header",
   "metadata": {},
   "source": [
    "## 2.3 Remove Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove-columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    def identify_unnecessary_columns(df):\n",
    "        \"\"\"\n",
    "        Identify columns that should be removed for prediction.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            list: List of columns to remove\n",
    "        \"\"\"\n",
    "        # Common patterns for unnecessary columns\n",
    "        unnecessary_patterns = [\n",
    "            'id', 'ID', 'customer_id', 'customerID', 'customerId',\n",
    "            'name', 'Name', 'email', 'phone', 'address'\n",
    "        ]\n",
    "        \n",
    "        columns_to_remove = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Check if column name matches unnecessary patterns\n",
    "            if any(pattern in col for pattern in unnecessary_patterns):\n",
    "                columns_to_remove.append(col)\n",
    "            # Check if column has too many unique values (likely an identifier)\n",
    "            elif df[col].nunique() == len(df) and col.lower() != 'churn':\n",
    "                columns_to_remove.append(col)\n",
    "        \n",
    "        return columns_to_remove\n",
    "    \n",
    "    # Identify unnecessary columns\n",
    "    unnecessary_cols = identify_unnecessary_columns(df_clean)\n",
    "    \n",
    "    if unnecessary_cols:\n",
    "        print(f\"🗑️ Identified columns to remove: {unnecessary_cols}\")\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        df_clean = df_clean.drop(columns=unnecessary_cols)\n",
    "        print(f\"✅ Removed {len(unnecessary_cols)} unnecessary columns\")\n",
    "    else:\n",
    "        print(\"✅ No unnecessary columns identified\")\n",
    "    \n",
    "    print(f\"\\n📊 Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"📋 Remaining columns: {list(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-header",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "\n",
    "Comprehensive analysis of the data including distributions, correlations, and churn patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target-analysis-header",
   "metadata": {},
   "source": [
    "## 3.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Identify target column\n",
    "    target_candidates = ['churn', 'Churn', 'CHURN', 'churned', 'Churned']\n",
    "    target_col = None\n",
    "    \n",
    "    for col in target_candidates:\n",
    "        if col in df_clean.columns:\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"🎯 Target column identified: {target_col}\")\n",
    "        \n",
    "        # Convert target to binary if needed\n",
    "        if df_clean[target_col].dtype == 'object':\n",
    "            # Map common churn representations to binary\n",
    "            churn_mapping = {\n",
    "                'Yes': 1, 'No': 0,\n",
    "                'True': 1, 'False': 0,\n",
    "                '1': 1, '0': 0,\n",
    "                True: 1, False: 0\n",
    "            }\n",
    "            df_clean[target_col] = df_clean[target_col].map(churn_mapping)\n",
    "        \n",
    "        # Analyze target distribution\n",
    "        churn_counts = df_clean[target_col].value_counts()\n",
    "        churn_percentages = df_clean[target_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\n📊 Churn Distribution:\")\n",
    "        print(f\"No Churn (0): {churn_counts[0]} ({churn_percentages[0]:.2f}%)\")\n",
    "        print(f\"Churn (1): {churn_counts[1]} ({churn_percentages[1]:.2f}%)\")\n",
    "        \n",
    "        # Visualize target distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Bar plot\n",
    "        axes[0].bar(churn_counts.index, churn_counts.values, \n",
    "                   color=['skyblue', 'lightcoral'])\n",
    "        axes[0].set_xlabel('Churn Status')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title('Churn Distribution')\n",
    "        axes[0].set_xticks([0, 1])\n",
    "        axes[0].set_xticklabels(['No Churn', 'Churn'])\n",
    "        \n",
    "        # Pie chart\n",
    "        axes[1].pie(churn_counts.values, labels=['No Churn', 'Churn'], \n",
    "                   colors=['skyblue', 'lightcoral'], autopct='%1.2f%%')\n",
    "        axes[1].set_title('Churn Distribution (Percentage)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Check for class imbalance\n",
    "        imbalance_ratio = churn_counts.min() / churn_counts.max()\n",
    "        if imbalance_ratio < 0.7:\n",
    "            print(f\"⚠️ Class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "            print(\"Consider using techniques like SMOTE for balancing during model training.\")\n",
    "        else:\n",
    "            print(\"✅ Classes are relatively balanced\")\n",
    "    else:\n",
    "        print(\"❌ Target column not found. Please ensure your dataset has a 'Churn' column.\")\n",
    "        print(\"Available columns:\", list(df_clean.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-features-header",
   "metadata": {},
   "source": [
    "## 3.2 Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Get numerical columns (excluding target)\n",
    "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numerical_cols:\n",
    "        numerical_cols.remove(target_col)\n",
    "    \n",
    "    if numerical_cols:\n",
    "        print(f\"📊 Numerical features: {numerical_cols}\")\n",
    "        \n",
    "        # Distribution plots\n",
    "        n_cols = min(3, len(numerical_cols))\n",
    "        n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif len(numerical_cols) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i, col in enumerate(numerical_cols):\n",
    "            row = i // n_cols\n",
    "            col_idx = i % n_cols\n",
    "            \n",
    "            if row < n_rows and col_idx < n_cols:\n",
    "                # Distribution by churn status\n",
    "                for churn_val in [0, 1]:\n",
    "                    data = df_clean[df_clean[target_col] == churn_val][col]\n",
    "                    axes[row, col_idx].hist(data, alpha=0.7, \n",
    "                                          label=f'Churn: {\"Yes\" if churn_val else \"No\"}')\n",
    "                \n",
    "                axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "                axes[row, col_idx].set_xlabel(col)\n",
    "                axes[row, col_idx].set_ylabel('Frequency')\n",
    "                axes[row, col_idx].legend()\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col_idx = i % n_cols\n",
    "            if row < n_rows and col_idx < n_cols:\n",
    "                fig.delaxes(axes[row, col_idx])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical summary by churn\n",
    "        print(\"\\n📈 Statistical Summary by Churn Status:\")\n",
    "        for col in numerical_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            summary = df_clean.groupby(target_col)[col].agg(['mean', 'median', 'std'])\n",
    "            display(summary)\n",
    "    else:\n",
    "        print(\"No numerical features found (excluding target).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical-features-header",
   "metadata": {},
   "source": [
    "## 3.3 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorical-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Get categorical columns\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"📝 Categorical features: {categorical_cols}\")\n",
    "        \n",
    "        # Analyze each categorical feature\n",
    "        for col in categorical_cols:\n",
    "            print(f\"\\n🔍 Analysis of {col}:\")\n",
    "            \n",
    "            # Cross-tabulation with churn\n",
    "            cross_tab = pd.crosstab(df_clean[col], df_clean[target_col], normalize='index') * 100\n",
    "            print(f\"Churn rate by {col}:\")\n",
    "            display(cross_tab.round(2))\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Count plot\n",
    "            sns.countplot(data=df_clean, x=col, hue=target_col, ax=axes[0])\n",
    "            axes[0].set_title(f'Count of {col} by Churn Status')\n",
    "            axes[0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Percentage plot\n",
    "            cross_tab.plot(kind='bar', ax=axes[1], color=['skyblue', 'lightcoral'])\n",
    "            axes[1].set_title(f'Churn Rate by {col}')\n",
    "            axes[1].set_ylabel('Churn Rate (%)')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "            axes[1].legend(['No Churn', 'Churn'])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No categorical features found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation-header",
   "metadata": {},
   "source": [
    "## 3.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Create a copy for correlation analysis\n",
    "    df_corr = df_clean.copy()\n",
    "    \n",
    "    # Encode categorical variables for correlation analysis\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = df_corr.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df_corr[col] = le.fit_transform(df_corr[col])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlations with target variable\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    target_correlations = target_correlations[target_correlations.index != target_col]\n",
    "    \n",
    "    print(f\"\\n🎯 Features most correlated with {target_col}:\")\n",
    "    display(target_correlations.head(10))\n",
    "    \n",
    "    # Plot top correlations with target\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_correlations = target_correlations.head(10)\n",
    "    colors = ['red' if corr > 0 else 'blue' for corr in correlation_matrix[target_col][top_correlations.index]]\n",
    "    plt.barh(range(len(top_correlations)), top_correlations.values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_correlations)), top_correlations.index)\n",
    "    plt.xlabel('Absolute Correlation with Churn')\n",
    "    plt.title('Top 10 Features Correlated with Churn')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering-header",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "Prepare features for machine learning by encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding-header",
   "metadata": {},
   "source": [
    "## 4.1 Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    def encode_categorical_features(df, target_col):\n",
    "        \"\"\"\n",
    "        Encode categorical features using appropriate methods.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            target_col (str): Target column name\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with encoded features\n",
    "        \"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        if not categorical_cols:\n",
    "            print(\"✅ No categorical columns to encode\")\n",
    "            return df_encoded\n",
    "        \n",
    "        print(f\"🔄 Encoding categorical features: {categorical_cols}\")\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            unique_values = df_encoded[col].nunique()\n",
    "            print(f\"  {col}: {unique_values} unique values\")\n",
    "            \n",
    "            if unique_values == 2:\n",
    "                # Binary encoding for binary categorical variables\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "                print(f\"    ✅ Applied Label Encoding\")\n",
    "            elif unique_values <= 5:\n",
    "                # One-hot encoding for low cardinality categorical variables\n",
    "                dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
    "                df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "                df_encoded.drop(col, axis=1, inplace=True)\n",
    "                print(f\"    ✅ Applied One-Hot Encoding ({len(dummies.columns)} new columns)\")\n",
    "            else:\n",
    "                # Label encoding for high cardinality categorical variables\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "                print(f\"    ✅ Applied Label Encoding (high cardinality)\")\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    # Encode categorical features\n",
    "    df_encoded = encode_categorical_features(df_clean, target_col)\n",
    "    \n",
    "    print(f\"\\n📊 Dataset shape after encoding: {df_encoded.shape}\")\n",
    "    print(f\"📋 Features after encoding: {len(df_encoded.columns) - 1} (excluding target)\")\n",
    "    \n",
    "    # Show data types after encoding\n",
    "    print(\"\\n📋 Data types after encoding:\")\n",
    "    for col in df_encoded.columns:\n",
    "        print(f\"  {col}: {df_encoded[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-header",
   "metadata": {},
   "source": [
    "## 4.2 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    def prepare_features_for_scaling(df, target_col):\n",
    "        \"\"\"\n",
    "        Prepare features and target for scaling.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            target_col (str): Target column name\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (X, y) features and target\n",
    "        \"\"\"\n",
    "        # Separate features and target\n",
    "        X = df.drop(target_col, axis=1)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_for_scaling(df_encoded, target_col)\n",
    "    \n",
    "    print(f\"📊 Features shape: {X.shape}\")\n",
    "    print(f\"🎯 Target shape: {y.shape}\")\n",
    "    print(f\"📋 Feature names: {list(X.columns)}\")\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Note: We'll apply scaling after train-test split to prevent data leakage\n",
    "    print(\"\\n✅ StandardScaler initialized (will be applied after train-test split)\")\n",
    "    \n",
    "    # Show feature statistics before scaling\n",
    "    print(\"\\n📈 Feature statistics before scaling:\")\n",
    "    display(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training-header",
   "metadata": {},
   "source": [
    "# 5. Model Training & Evaluation\n",
    "\n",
    "Split data, train models, and evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-splitting-header",
   "metadata": {},
   "source": [
    "## 5.1 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-splitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Data split completed:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Check class distribution in splits\n",
    "    train_churn_dist = y_train.value_counts(normalize=True) * 100\n",
    "    test_churn_dist = y_test.value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\n📊 Class distribution:\")\n",
    "    print(f\"Training set - No Churn: {train_churn_dist[0]:.2f}%, Churn: {train_churn_dist[1]:.2f}%\")\n",
    "    print(f\"Test set - No Churn: {test_churn_dist[0]:.2f}%, Churn: {test_churn_dist[1]:.2f}%\")\n",
    "    \n",
    "    # Apply scaling\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrames for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\n✅ Feature scaling applied\")\n",
    "    print(f\"📈 Scaled features statistics:\")\n",
    "    display(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class-balancing-header",
   "metadata": {},
   "source": [
    "## 5.2 Handle Class Imbalance (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class-balancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Check if we need to handle class imbalance\n",
    "    class_counts = y_train.value_counts()\n",
    "    imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "    \n",
    "    if imbalance_ratio < 0.7:\n",
    "        print(f\"⚖️ Applying SMOTE to handle class imbalance (ratio: {imbalance_ratio:.2f})\")\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=RANDOM_STATE)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        X_train_balanced = pd.DataFrame(X_train_balanced, columns=X.columns)\n",
    "        \n",
    "        print(f\"✅ SMOTE applied:\")\n",
    "        print(f\"  Original training size: {len(X_train_scaled)}\")\n",
    "        print(f\"  Balanced training size: {len(X_train_balanced)}\")\n",
    "        \n",
    "        # Show new class distribution\n",
    "        balanced_dist = pd.Series(y_train_balanced).value_counts(normalize=True) * 100\n",
    "        print(f\"  New distribution - No Churn: {balanced_dist[0]:.2f}%, Churn: {balanced_dist[1]:.2f}%\")\n",
    "        \n",
    "        # Use balanced data for training\n",
    "        X_train_final = X_train_balanced\n",
    "        y_train_final = y_train_balanced\n",
    "    else:\n",
    "        print(f\"✅ Classes are reasonably balanced (ratio: {imbalance_ratio:.2f})\")\n",
    "        X_train_final = X_train_scaled\n",
    "        y_train_final = y_train\n",
    "    \n",
    "    print(f\"\\n📊 Final training data shape: {X_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-selection-header",
   "metadata": {},
   "source": [
    "## 5.3 Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Define models to compare\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    model_results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"🚀 Training models...\\n\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_final, y_train_final)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Store results\n",
    "        model_results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        \n",
    "        print(f\"  ✅ {name} trained - F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Create results summary\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(model_results.keys()),\n",
    "        'Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "        'Precision': [results['precision'] for results in model_results.values()],\n",
    "        'Recall': [results['recall'] for results in model_results.values()],\n",
    "        'F1-Score': [results['f1_score'] for results in model_results.values()],\n",
    "        'AUC': [results['auc'] for results in model_results.values()]\n",
    "    })\n",
    "    \n",
    "    # Sort by F1-score\n",
    "    results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n📊 Model Comparison Results:\")\n",
    "    display(results_df.round(4))\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\n🏆 Best performing model: {best_model_name}\")\n",
    "    print(f\"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter-tuning-header",
   "metadata": {},
   "source": [
    "## 5.4 Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Define parameter grids for different models\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.1, 0.2],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.1, 0.2],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'Logistic Regression': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if best_model_name in param_grids:\n",
    "        print(f\"🔧 Performing hyperparameter tuning for {best_model_name}...\")\n",
    "        \n",
    "        # Get the base model\n",
    "        if best_model_name == 'Random Forest':\n",
    "            base_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "        elif best_model_name == 'Gradient Boosting':\n",
    "            base_model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "        elif best_model_name == 'XGBoost':\n",
    "            base_model = xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "        else:\n",
    "            base_model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model,\n",
    "            param_grids[best_model_name],\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_final, y_train_final)\n",
    "        \n",
    "        # Get best model\n",
    "        best_tuned_model = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"\\n✅ Hyperparameter tuning completed\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best CV F1-score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Evaluate tuned model on test set\n",
    "        y_pred_tuned = best_tuned_model.predict(X_test_scaled)\n",
    "        y_pred_proba_tuned = best_tuned_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "        tuned_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "        \n",
    "        print(f\"\\n📈 Tuned model performance on test set:\")\n",
    "        print(f\"F1-Score: {tuned_f1:.4f} (vs {model_results[best_model_name]['f1_score']:.4f} before tuning)\")\n",
    "        print(f\"AUC: {tuned_auc:.4f} (vs {model_results[best_model_name]['auc']:.4f} before tuning)\")\n",
    "        \n",
    "        # Use tuned model as final model if it's better\n",
    "        if tuned_f1 > model_results[best_model_name]['f1_score']:\n",
    "            final_model = best_tuned_model\n",
    "            final_predictions = y_pred_tuned\n",
    "            final_probabilities = y_pred_proba_tuned\n",
    "            print(\"🏆 Using tuned model as final model\")\n",
    "        else:\n",
    "            final_model = best_model\n",
    "            final_predictions = model_results[best_model_name]['predictions']\n",
    "            final_probabilities = model_results[best_model_name]['probabilities']\n",
    "            print(\"📊 Original model performs better, using original model\")\n",
    "    else:\n",
    "        final_model = best_model\n",
    "        final_predictions = model_results[best_model_name]['predictions']\n",
    "        final_probabilities = model_results[best_model_name]['probabilities']\n",
    "        print(f\"⚠️ No hyperparameter grid defined for {best_model_name}, using original model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-insights-header",
   "metadata": {},
   "source": [
    "# 6. Results & Insights\n",
    "\n",
    "Comprehensive evaluation of the final model and business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-report-header",
   "metadata": {},
   "source": [
    "## 6.1 Detailed Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Calculate comprehensive metrics\n",
    "    final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "    final_precision = precision_score(y_test, final_predictions)\n",
    "    final_recall = recall_score(y_test, final_predictions)\n",
    "    final_f1 = f1_score(y_test, final_predictions)\n",
    "    final_auc = roc_auc_score(y_test, final_probabilities)\n",
    "    \n",
    "    # Create performance report\n",
    "    print(\"🎯 FINAL MODEL PERFORMANCE REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Model: {best_model_name}\")\n",
    "    print(f\"Dataset: {len(y_test)} test samples\")\n",
    "    print(\"\\n📊 Classification Metrics:\")\n",
    "    print(f\"  • Accuracy:  {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "    print(f\"  • Precision: {final_precision:.4f} ({final_precision*100:.2f}%)\")\n",
    "    print(f\"  • Recall:    {final_recall:.4f} ({final_recall*100:.2f}%)\")\n",
    "    print(f\"  • F1-Score:  {final_f1:.4f} ({final_f1*100:.2f}%)\")\n",
    "    print(f\"  • AUC-ROC:   {final_auc:.4f} ({final_auc*100:.2f}%)\")\n",
    "    \n",
    "    # Interpretation guide\n",
    "    print(\"\\n📖 Metric Interpretation:\")\n",
    "    print(f\"  • Accuracy: {final_accuracy*100:.1f}% of predictions are correct\")\n",
    "    print(f\"  • Precision: {final_precision*100:.1f}% of predicted churners actually churned\")\n",
    "    print(f\"  • Recall: {final_recall*100:.1f}% of actual churners were correctly identified\")\n",
    "    print(f\"  • F1-Score: Balanced measure of precision and recall\")\n",
    "    print(f\"  • AUC-ROC: Model's ability to distinguish between classes\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n📋 Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, final_predictions, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-header",
   "metadata": {},
   "source": [
    "## 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, final_predictions)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # Normalized confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'],\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('Normalized Confusion Matrix')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed confusion matrix analysis\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(\"🔍 CONFUSION MATRIX ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"True Negatives (TN):  {tn:4d} - Correctly predicted no churn\")\n",
    "    print(f\"False Positives (FP): {fp:4d} - Incorrectly predicted churn\")\n",
    "    print(f\"False Negatives (FN): {fn:4d} - Missed churn cases\")\n",
    "    print(f\"True Positives (TP):  {tp:4d} - Correctly predicted churn\")\n",
    "    \n",
    "    # Business impact analysis\n",
    "    print(\"\\n💼 BUSINESS IMPACT:\")\n",
    "    print(f\"  • Successfully retained customers: {tn} (avoided churn)\")\n",
    "    print(f\"  • Unnecessary retention efforts: {fp} (false alarms)\")\n",
    "    print(f\"  • Missed churn opportunities: {fn} (lost customers)\")\n",
    "    print(f\"  • Successful churn predictions: {tp} (intervention opportunities)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc-curve-header",
   "metadata": {},
   "source": [
    "## 6.3 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, final_probabilities)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {final_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random Classifier (AUC = 0.5)')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curve for imbalanced datasets\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, final_probabilities)\n",
    "    avg_precision = average_precision_score(y_test, final_probabilities)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(recall_curve, precision_curve, color='darkgreen', lw=2,\n",
    "             label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "    plt.axhline(y=y_test.mean(), color='navy', linestyle='--', \n",
    "                label=f'Baseline (AP = {y_test.mean():.3f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"📈 ROC & PR Curve Analysis:\")\n",
    "    print(f\"  • AUC-ROC: {final_auc:.3f} - {'Excellent' if final_auc > 0.9 else 'Good' if final_auc > 0.8 else 'Fair' if final_auc > 0.7 else 'Poor'}\")\n",
    "    print(f\"  • Average Precision: {avg_precision:.3f}\")\n",
    "    print(f\"  • The model performs {'well' if final_auc > 0.75 else 'adequately'} at distinguishing churners from non-churners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## 6.4 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    def get_feature_importance(model, feature_names):\n",
    "        \"\"\"\n",
    "        Extract feature importance from different model types.\n",
    "        \"\"\"\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Tree-based models\n",
    "            return model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            # Linear models\n",
    "            return np.abs(model.coef_[0])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = get_feature_importance(final_model, X.columns)\n",
    "    \n",
    "    if importance is not None:\n",
    "        # Create feature importance DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Display top features\n",
    "        print(\"🎯 TOP 10 MOST IMPORTANT FEATURES FOR CHURN PREDICTION\")\n",
    "        print(\"=\" * 60)\n",
    "        display(feature_importance_df.head(10))\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance_df.head(15)\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(data=top_features, x='Importance', y='Feature', palette='viridis')\n",
    "        plt.title('Top 15 Feature Importance')\n",
    "        plt.xlabel('Importance Score')\n",
    "        \n",
    "        # Cumulative importance\n",
    "        cumulative_importance = feature_importance_df['Importance'].cumsum() / feature_importance_df['Importance'].sum()\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'bo-')\n",
    "        plt.axhline(y=0.8, color='r', linestyle='--', label='80% Variance')\n",
    "        plt.axhline(y=0.9, color='orange', linestyle='--', label='90% Variance')\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Cumulative Importance')\n",
    "        plt.title('Cumulative Feature Importance')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find number of features for 80% importance\n",
    "        features_80 = (cumulative_importance >= 0.8).argmax() + 1\n",
    "        features_90 = (cumulative_importance >= 0.9).argmax() + 1\n",
    "        \n",
    "        print(f\"\\n📊 Feature Analysis:\")\n",
    "        print(f\"  • {features_80} features explain 80% of model importance\")\n",
    "        print(f\"  • {features_90} features explain 90% of model importance\")\n",
    "        print(f\"  • Total features used: {len(X.columns)}\")\n",
    "        \n",
    "        # Business insights from top features\n",
    "        print(\"\\n💡 KEY BUSINESS INSIGHTS:\")\n",
    "        print(\"Top factors contributing to customer churn:\")\n",
    "        for i, (_, row) in enumerate(feature_importance_df.head(5).iterrows(), 1):\n",
    "            print(f\"  {i}. {row['Feature']}: {row['Importance']:.4f} importance\")\n",
    "    else:\n",
    "        print(\"⚠️ Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business-insights-header",
   "metadata": {},
   "source": [
    "## 6.5 Business Recommendations & Action Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "business-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    print(\"💼 BUSINESS RECOMMENDATIONS & ACTION PLAN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Model deployment recommendations\n",
    "    print(\"\\n🚀 MODEL DEPLOYMENT:\")\n",
    "    print(f\"  • Model Readiness: {'Ready for production' if final_f1 > 0.7 else 'Needs improvement'}\")\n",
    "    print(f\"  • Expected Performance: {final_f1*100:.1f}% F1-score on new data\")\n",
    "    print(f\"  • Recommended Threshold: 0.5 (adjustable based on business priorities)\")\n",
    "    \n",
    "    # Customer segmentation based on churn risk\n",
    "    churn_probabilities = final_probabilities\n",
    "    high_risk = (churn_probabilities > 0.7).sum()\n",
    "    medium_risk = ((churn_probabilities > 0.3) & (churn_probabilities <= 0.7)).sum()\n",
    "    low_risk = (churn_probabilities <= 0.3).sum()\n",
    "    \n",
    "    print(\"\\n👥 CUSTOMER RISK SEGMENTATION:\")\n",
    "    print(f\"  • High Risk (>70% churn probability): {high_risk} customers\")\n",
    "    print(f\"  • Medium Risk (30-70% churn probability): {medium_risk} customers\")\n",
    "    print(f\"  • Low Risk (<30% churn probability): {low_risk} customers\")\n",
    "    \n",
    "    # Action recommendations by risk level\n",
    "    print(\"\\n🎯 RECOMMENDED ACTIONS BY RISK LEVEL:\")\n",
    "    print(\"\\n  HIGH RISK CUSTOMERS:\")\n",
    "    print(\"    - Immediate outreach by customer success team\")\n",
    "    print(\"    - Personalized retention offers or discounts\")\n",
    "    print(\"    - Priority customer support\")\n",
    "    print(\"    - Executive-level intervention if high-value customers\")\n",
    "    \n",
    "    print(\"\\n  MEDIUM RISK CUSTOMERS:\")\n",
    "    print(\"    - Proactive engagement campaigns\")\n",
    "    print(\"    - Product feature education\")\n",
    "    print(\"    - Satisfaction surveys and feedback collection\")\n",
    "    print(\"    - Loyalty program enrollment\")\n",
    "    \n",
    "    print(\"\\n  LOW RISK CUSTOMERS:\")\n",
    "    print(\"    - Standard communication cadence\")\n",
    "    print(\"    - Upselling and cross-selling opportunities\")\n",
    "    print(\"    - Referral program participation\")\n",
    "    print(\"    - Case study and testimonial opportunities\")\n",
    "    \n",
    "    # ROI estimation\n",
    "    print(\"\\n💰 ESTIMATED BUSINESS IMPACT:\")\n",
    "    tp_rate = final_recall\n",
    "    print(f\"  • Churn Detection Rate: {tp_rate*100:.1f}%\")\n",
    "    print(f\"  • If avg customer value is $1000 and 50% retention success:\")\n",
    "    print(f\"    - Potential value saved per month: ${int(tp * 1000 * 0.5):,}\")\n",
    "    print(f\"    - Annual potential impact: ${int(tp * 1000 * 0.5 * 12):,}\")\n",
    "    \n",
    "    # Model monitoring recommendations\n",
    "    print(\"\\n🔍 MODEL MONITORING & MAINTENANCE:\")\n",
    "    print(\"  • Retrain model monthly with new data\")\n",
    "    print(\"  • Monitor model performance metrics weekly\")\n",
    "    print(\"  • Track prediction accuracy vs actual churn\")\n",
    "    print(\"  • Alert if model performance drops below 70% F1-score\")\n",
    "    print(\"  • A/B test different thresholds for optimal business outcomes\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(\"\\n📋 NEXT STEPS:\")\n",
    "    print(\"  1. Validate model performance with business stakeholders\")\n",
    "    print(\"  2. Implement model in production environment\")\n",
    "    print(\"  3. Set up automated scoring pipeline\")\n",
    "    print(\"  4. Create dashboard for monitoring churn predictions\")\n",
    "    print(\"  5. Train customer success team on using predictions\")\n",
    "    print(\"  6. Establish feedback loop for model improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-saving-header",
   "metadata": {},
   "source": [
    "## 6.6 Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-saving",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and target_col:\n",
    "    # Save the trained model\n",
    "    model_filename = '../models/churn_prediction_model.pkl'\n",
    "    scaler_filename = '../models/feature_scaler.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Save model\n",
    "        joblib.dump(final_model, model_filename)\n",
    "        print(f\"✅ Model saved to {model_filename}\")\n",
    "        \n",
    "        # Save scaler\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        print(f\"✅ Feature scaler saved to {scaler_filename}\")\n",
    "        \n",
    "        # Save feature names for future use\n",
    "        feature_names_file = '../models/feature_names.pkl'\n",
    "        joblib.dump(list(X.columns), feature_names_file)\n",
    "        print(f\"✅ Feature names saved to {feature_names_file}\")\n",
    "        \n",
    "        # Save results summary\n",
    "        results_summary = {\n",
    "            'model_name': best_model_name,\n",
    "            'accuracy': final_accuracy,\n",
    "            'precision': final_precision,\n",
    "            'recall': final_recall,\n",
    "            'f1_score': final_f1,\n",
    "            'auc': final_auc,\n",
    "            'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'training_samples': len(X_train_final),\n",
    "            'test_samples': len(X_test),\n",
    "            'features_count': len(X.columns)\n",
    "        }\n",
    "        \n",
    "        results_file = '../results/model_performance_summary.pkl'\n",
    "        joblib.dump(results_summary, results_file)\n",
    "        print(f\"✅ Results summary saved to {results_file}\")\n",
    "        \n",
    "        # Create predictions CSV for business use\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'actual_churn': y_test.values,\n",
    "            'predicted_churn': final_predictions,\n",
    "            'churn_probability': final_probabilities,\n",
    "            'risk_level': pd.cut(final_probabilities, \n",
    "                               bins=[0, 0.3, 0.7, 1.0], \n",
    "                               labels=['Low', 'Medium', 'High'])\n",
    "        }, index=y_test.index)\n",
    "        \n",
    "        predictions_file = '../results/churn_predictions.csv'\n",
    "        predictions_df.to_csv(predictions_file)\n",
    "        print(f\"✅ Test predictions saved to {predictions_file}\")\n",
    "        \n",
    "        if importance is not None:\n",
    "            feature_importance_file = '../results/feature_importance.csv'\n",
    "            feature_importance_df.to_csv(feature_importance_file, index=False)\n",
    "            print(f\"✅ Feature importance saved to {feature_importance_file}\")\n",
    "        \n",
    "        print(\"\\n🎉 All files saved successfully!\")\n",
    "        print(\"\\n📁 Saved files:\")\n",
    "        print(f\"  • Model: {model_filename}\")\n",
    "        print(f\"  • Scaler: {scaler_filename}\")\n",
    "        print(f\"  • Features: {feature_names_file}\")\n",
    "        print(f\"  • Results: {results_file}\")\n",
    "        print(f\"  • Predictions: {predictions_file}\")\n",
    "        if importance is not None:\n",
    "            print(f\"  • Feature Importance: {feature_importance_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving files: {str(e)}\")\n",
    "        print(\"Please check that the models and results directories exist and are writable.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🏁 CUSTOMER CHURN PREDICTION PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final Model: {best_model_name}\")\n",
    "    print(f\"Performance: F1-Score = {final_f1:.3f}, AUC = {final_auc:.3f}\")\n",
    "    print(\"Ready for deployment and business integration!\")\n",
    "else:\n",
    "    print(\"\\n📝 To run this complete pipeline:\")\n",
    "    print(\"1. Place your customer_data.csv in the data/ directory\")\n",
    "    print(\"2. Ensure the CSV has a 'Churn' column (Yes/No or 1/0)\")\n",
    "    print(\"3. Run all cells from the beginning\")\n",
    "    print(\"\\nThe pipeline will automatically:\")\n",
    "    print(\"• Load and clean your data\")\n",
    "    print(\"• Perform comprehensive EDA\")\n",
    "    print(\"• Train multiple ML models\")\n",
    "    print(\"• Select and tune the best model\")\n",
    "    print(\"• Generate business insights and recommendations\")\n",
    "    print(\"• Save the final model for deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}